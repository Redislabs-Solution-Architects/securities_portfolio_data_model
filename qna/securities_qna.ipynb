{
  "cells": [
    {
      "cell_type": "code",
      "id": "HqRohfRRZgFWtj1zYJIyvR5n",
      "metadata": {
        "tags": [],
        "id": "HqRohfRRZgFWtj1zYJIyvR5n"
      },
      "source": [
        "!pwd\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Install required libraries\n",
        "!python3 -m pip -q install redis\n",
        "!pip install -U langchain gradio\n",
        "!pip install -U langchain-google-vertexai\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment & execute the following code in case Redis Enterprise is not available\n",
        "##################################################################################\n",
        "\n",
        "# %%sh\n",
        "# curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "# echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "# sudo apt-get update  > /dev/null 2>&1\n",
        "# sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "# redis-stack-server --daemonize yes"
      ],
      "metadata": {
        "id": "1zTK0KXJUwzX"
      },
      "id": "1zTK0KXJUwzX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Update the 'host' field with the correct Redis host URL\n",
        "host = ''\n",
        "port = \n",
        "password = ''\n",
        "requirePass = True\n",
        "\n",
        "## For redis-stack-server, comment out the above code and uncomment the following:\n",
        "# host = 'localhost'\n",
        "# requirePass = False"
      ],
      "metadata": {
        "id": "sjUooyI9VlAu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726214466105,
          "user_tz": -330,
          "elapsed": 623,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "sjUooyI9VlAu",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "\n",
        "if requirePass:\n",
        "    client = redis.Redis(host = host, port=port, decode_responses=True, password=password)\n",
        "else:\n",
        "    client = redis.Redis(host = 'localhost', decode_responses=True)\n",
        "\n",
        "print(client.ping())\n",
        "# Clear Redis database (optional)\n",
        "client.flushdb()\n",
        "\n",
        "REDIS_URL = f\"redis://:{password}@{host}:{port}\"\n",
        "INDEX_NAME = f\"idx_qna\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UnZUjIFVxWA",
        "outputId": "8f493f4b-fc52-48af-bbf6-eafe37b729b2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726214470732,
          "user_tz": -330,
          "elapsed": 2000,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "4UnZUjIFVxWA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Authenticate with GCP & set project id and region\n",
        "from google.colab import auth\n",
        "from getpass import getpass\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "# input your GCP project ID and region for Vertex AI\n",
        "PROJECT_ID = getpass(\"PROJECT_ID:\")\n",
        "REGION = 'us-central1' #input(\"REGION:\")\n",
        "\n",
        "print(f'PROJECT_ID: {PROJECT_ID} & REGION: {REGION}')\n"
      ],
      "metadata": {
        "id": "_38Ye7niWPZR"
      },
      "id": "_38Ye7niWPZR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/abhi-data-2024/how_india_shops_online.pdf -O report.pdf\n"
      ],
      "metadata": {
        "id": "cnEl0UcxWV5w"
      },
      "id": "cnEl0UcxWV5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "file = \"report.pdf\"\n",
        "\n",
        "# set up the file loader/extractor and text splitter to create chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2500, chunk_overlap=50, add_start_index=True\n",
        ")\n",
        "\n",
        "loader = PyPDFLoader(file)\n",
        "documents = loader.load()\n",
        "\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "#chunked_docs = [doc.page_content for doc in chunks]"
      ],
      "metadata": {
        "id": "-B_EHwP7ouXf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726214486345,
          "user_tz": -330,
          "elapsed": 5186,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "-B_EHwP7ouXf",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create text embeddings with Vertex AI embedding model\n",
        "\n",
        "Use the Vertex AI API for text embeddings, developed by Google.\n",
        "\n",
        "Text embeddings are a dense vector representation of a piece of content such that, if two pieces of content are semantically similar, their respective embeddings are located near each other in the embedding vector space. This representation can be used to solve common NLP tasks, such as:\n",
        "\n",
        "\n",
        "*   Semantic search: Search text ranked by semantic similarity.\n",
        "*   Recommendation: Return items with text attributes similar to the given text.\n",
        "*   Classification: Return the class of items whose text attributes are similar to the given text.\n",
        "*   Clustering: Cluster items whose text attributes are similar to the given text.\n",
        "*   Outlier Detection: Return items where text attributes are least related to the given text.\n",
        "\n",
        "The Vertex AI text-embeddings API lets you create a text embedding using Generative AI on Vertex AI. The textembedding-gecko model accepts a maximum of 3,072 input tokens (i.e. words) and outputs 768-dimensional vector embeddings."
      ],
      "metadata": {
        "id": "hKHXwtSGwNPr"
      },
      "id": "hKHXwtSGwNPr"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.redis import Redis\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\", project=PROJECT_ID, location=REGION)\n",
        "\n",
        "def get_vectordb() -> Redis:\n",
        "    \"\"\"Create the Redis vectordb.\"\"\"\n",
        "\n",
        "    try:\n",
        "        vectordb = Redis.from_existing_index(\n",
        "            embedding=embeddings,\n",
        "            index_name=INDEX_NAME,\n",
        "            redis_url=REDIS_URL\n",
        "        )\n",
        "        return vectordb\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Load Redis with documents\n",
        "    vectordb = Redis.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "        redis_url=REDIS_URL\n",
        "    )\n",
        "    return vectordb\n",
        "\n",
        "\n",
        "redis = get_vectordb()\n",
        "\n",
        "#embedder = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
      ],
      "metadata": {
        "id": "A6H_vHkrSxco",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726214604049,
          "user_tz": -330,
          "elapsed": 4653,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "A6H_vHkrSxco",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Include RAG\n",
        "\n",
        "We're going to build a complete RAG pipeline from scratch incorporating the following components:\n",
        "\n",
        "Standard retrieval and chat completion\n",
        "Dense content representation to improve accuracy\n",
        "Query re-writing to improve accuracy\n",
        "Semantic caching to improve performance\n",
        "Conversational session history to improve personalization"
      ],
      "metadata": {
        "id": "kVJdyD1OVYE-"
      },
      "id": "kVJdyD1OVYE-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Prompt template\n",
        "PromptTemplate defines the exect text of the response that would be fed to the LLM. This step is optional, but the defaults usually work well for OpenAI and might fall short for other models."
      ],
      "metadata": {
        "id": "aNtLTGcvR-IS"
      },
      "id": "aNtLTGcvR-IS"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to define prompt template\n",
        "\n",
        "def create_prompt():\n",
        "    \"\"\"Create the QA chain.\"\"\"\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains import RetrievalQA\n",
        "\n",
        "    # Define our prompt\n",
        "    prompt_template = \"\"\"Use only the following pieces of context to answer the question. If you don't know the answer, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    This should be in the following format:\n",
        "\n",
        "    Question: [question here]\n",
        "    Answer: [answer here]\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Context:\n",
        "    ---------\n",
        "    {context}\n",
        "    ---------\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "BCtEFE8ziT5d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726214843581,
          "user_tz": -330,
          "elapsed": 608,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "BCtEFE8ziT5d",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Invoke Google Vertex LLM using Langchain\n",
        "# This is where the Langchain brings all the components together in a form of a simple QnA chain\n",
        "from langchain_google_vertexai import VertexAI\n",
        "\n",
        "llm = VertexAI(\n",
        "    model_name=\"gemini-1.5-pro-preview-0409\",\n",
        "    max_output_tokens=2048,\n",
        "    temperature=0.5,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=redis.as_retriever(search_type=\"similarity_distance_threshold\",search_kwargs={\"distance_threshold\":0.5}),\n",
        "    #return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": create_prompt()},\n",
        "    #verbose=True\n",
        "    )"
      ],
      "metadata": {
        "id": "DYtdUxxFSxh9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726214858231,
          "user_tz": -330,
          "elapsed": 632,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "DYtdUxxFSxh9",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are some motivations for shopping online?')['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "XelSUbTkS7rj",
        "outputId": "f4c5c31b-31bc-436e-9435-863f36fc9537",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726215050890,
          "user_tz": -330,
          "elapsed": 2628,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "XelSUbTkS7rj",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Some motivators for shopping online include the absence of physical stores for premium brands, stockouts of certain products, and a lack of knowledgeable staff in offline stores.  Additional motivators include the lack of discounts and special offers in physical stores, along with large crowds in malls during weekends. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('How do Indians like to pay for shopping online?')['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "B2f9CC3JI2BL",
        "outputId": "aed728f4-b2ad-403e-eaf6-17edbdf098f2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726215057332,
          "user_tz": -330,
          "elapsed": 3268,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "B2f9CC3JI2BL",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Urban dwellers and those who live in the rest of India are similar in their preference for using UPI for online payments. However, cash on delivery is still the most preferred option among those in the rest of India. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are some known challenges in shopping online?')['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "LLHoWVshJJHK",
        "outputId": "0175c171-4f92-4a23-fefe-c1ecbd311d6c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726215120585,
          "user_tz": -330,
          "elapsed": 2903,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "LLHoWVshJJHK",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question: What are some known challenges in shopping online?\\nAnswer: Some known challenges of shopping online are payment fraud, credibility of unfamiliar websites, and doubts regarding product quality matching the images shown.  Also, urban delivery faces logistical overload, regulations, and access restrictions, causing inconsistent experiences to the consumers. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('How home and kitchen segment is growing?')['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "5PVFYBW1NNKm",
        "outputId": "7e1e0cd7-3c24-4d5a-a718-d78112c29dbb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726215133369,
          "user_tz": -330,
          "elapsed": 2388,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "5PVFYBW1NNKm",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Question: How home and kitchen segment is growing?\\nAnswer: In the Indian market, the home furnishing and kitchen sector is growing at a CAGR of 10%. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are the effects of social media on online shopping?')['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "bgfyRlzCNquj",
        "outputId": "6f6970fc-5467-4bd7-fa90-74997301ab4a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726215146560,
          "user_tz": -330,
          "elapsed": 2331,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "bgfyRlzCNquj",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Answer: Social media has amplified awareness and aspirations of products. 62% of respondents tried products after seeing them on Facebook and Instagram. Social media is the most preferred channel for encouraging trials of new products. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa.invoke('What are some relevant items that are shopped online?')['result']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xClDk__XHFw-",
        "outputId": "df24cd18-7e3e-47ff-f591-15a38898a2ca",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726215152962,
          "user_tz": -330,
          "elapsed": 2059,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "xClDk__XHFw-",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'    Answer: Grocery, Fashion and Accessories, Electronics and Consumer Durables, Beauty and Personal Care, Health and Wellness, Home and Kitchen, Sports and Fitness \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def handle(query):\n",
        "    response = qa.run(query)\n",
        "    return response\n",
        "\n",
        "iface = gr.Interface(fn=handle, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "UmsYlwP9XZ2G"
      },
      "id": "UmsYlwP9XZ2G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvmnrq_3YY6p",
        "outputId": "2eecf130-23be-4791-e8f9-0d0e28eb2e7d"
      },
      "id": "nvmnrq_3YY6p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "GCP_GenAI_demo.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
